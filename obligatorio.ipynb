{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "\n",
    "df_test = pd.read_csv(\"./new_classification_test.csv\", sep = \",\")\n",
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de test\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de train\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los datos de train tienen una columna más respecto a los datos de test. Esta columna es la de la variable a predecir (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el dataset está formado de 134.571 observaciones. \n",
    "En total, hay 37.743 celdas sin datos, por lo que vamos a tener que tener esto en cuenta.\n",
    "\n",
    "La primera columna es un id, por lo que la podemos borrar, ya que no aporta nada de información, debido a que es un índice.\n",
    "\n",
    "La segunda columna, es el headline. Hay 133.726 datos distintos y 3 faltantes, por lo que podemos concluir que hay algunos headlines repetidos en el dataset.\n",
    "\n",
    "Algo similar ocurre con la columna authors. Solo el 20% de los datos son distintos, por lo que hay autores que se repiten. Esto puede ser útil porque tal vez un autor tenga preferencia a escribir noticias de una categoría en particular. Además, un dato no menor es que faltan 24.477 datos de autores. \n",
    "\n",
    "En cuanto a la categoria short_description, podemos observar que en su gran mayoría son distintos, pero hay algunas descripciones que se repiten. Además, también hay un gran número de valores faltantes (13.263).\n",
    "\n",
    "De la columna date podemos decir que hay muchos datos repetidos, ya que solo e 1.7% son valores distintos. Además, no falta ningún valor.\n",
    "\n",
    "Por último, la columna category, es la de la variable a predecir, Hay 41 categorías distintas en todo el set de datos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenamos datos faltantes:\n",
    "df_train['headline'].fillna('Unknown headline', inplace=True)\n",
    "df_test['headline'].fillna('Unknown headline', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['short_description'].fillna('Unknown short description', inplace=True)\n",
    "df_test['short_description'].fillna('Unknown short description', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['authors'].fillna('Unknown authors', inplace=True)\n",
    "df_test['authors'].fillna('Unknown authors', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columna de id\n",
    "# Quitamos la columna id del dataframe porque vimos que era un id autoincremental, el cual era lo mismo que el índice de la fila, por lo tanto, no aportaba información.\n",
    "df_train.drop(columns=['id'], axis=1, inplace=True)\n",
    "df_test.drop(columns=['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos el tipo de los datos a string, ya que antes eran de tipo object, pero en realidad, todos deberían ser strings\n",
    "df_train[['headline', 'authors', 'short_description', 'category', 'date']] = df_train[['headline', 'authors', 'short_description', 'category', 'date']].astype('string')\n",
    "df_test[['headline', 'authors', 'short_description', 'date']] = df_test[['headline', 'authors', 'short_description', 'date']].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 2309 fechas distintas en el dataset\n",
    "df_train['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 41 categorias distintas en el dataset\n",
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "df_train['category'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countplot por categoria\n",
    "plt.figure(figsize=(50,5))\n",
    "sns.countplot(data=df_train, x='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la mayoría de noticias son de la categoría política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma nos da una noción global de como es la distribución de \"category\". Los valores más frecuentes son:\n",
    "\n",
    "1 - Politics (21935)\n",
    "\n",
    "2 - Wellness (11944)\n",
    "\n",
    "3 - Entertainment (10759)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_corpus = list(df_train['headline'])\n",
    "headlines_corpus_test = list(df_test['headline'])\n",
    "\n",
    "headline_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus)\n",
    "headline_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headline_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headlines = headline_vectorizer.transform(headlines_corpus)\n",
    "X_test_headlines = headline_vectorizer_test.transform(headlines_corpus_test)\n",
    "print(X_train_headlines.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_description_corpus = list(df_train['short_description'])\n",
    "short_description_corpus_test = list(df_test['short_description'])\n",
    "\n",
    "shortDescription_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus)\n",
    "shortDescription_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortDescription_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shortDescription_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short_description = shortDescription_vectorizer.transform(short_description_corpus)\n",
    "X_test_short_description = shortDescription_vectorizer_test.transform(short_description_corpus_test)\n",
    "print(X_train_short_description.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_corpus = list(df_train['authors'])\n",
    "authors_corpus_test = list(df_test['authors'])\n",
    "\n",
    "authors_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus)\n",
    "authors_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_authors = authors_vectorizer.transform(authors_corpus)\n",
    "X_test_authors = authors_vectorizer_test.transform(authors_corpus_test)\n",
    "print(X_train_authors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora concatenamos los vectores de headlines, short_description y authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "headline_feature_names = ['headline' + elem for elem in headline_vectorizer.get_feature_names()]\n",
    "shortDescription_feature_names = ['shortDescription' + elem for elem in shortDescription_vectorizer.get_feature_names()]\n",
    "authors_feature_names = ['authors' + elem for elem in authors_vectorizer.get_feature_names()]\n",
    "dfHeadline = pd.DataFrame.sparse.from_spmatrix(X_train_headlines, columns=headline_feature_names)\n",
    "dfShortDescription = pd.DataFrame.sparse.from_spmatrix(X_train_short_description, columns=shortDescription_feature_names)\n",
    "dfAuthors = pd.DataFrame.sparse.from_spmatrix(X_train_authors, columns=authors_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "headline_feature_names_test = ['headline' + elem for elem in headline_vectorizer_test.get_feature_names()]\n",
    "shortDescription_feature_names_test = ['shortDescription' + elem for elem in shortDescription_vectorizer_test.get_feature_names()]\n",
    "authors_feature_names_test = ['authors' + elem for elem in authors_vectorizer_test.get_feature_names()]\n",
    "dfHeadline_test = pd.DataFrame.sparse.from_spmatrix(X_test_headlines, columns=headline_feature_names_test)\n",
    "dfShortDescription_test = pd.DataFrame.sparse.from_spmatrix(X_test_short_description, columns=shortDescription_feature_names_test)\n",
    "dfAuthors_test = pd.DataFrame.sparse.from_spmatrix(X_test_authors, columns=authors_feature_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAuthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, dfHeadline, dfShortDescription, dfAuthors], axis=1)\n",
    "df_test = pd.concat([df_test, dfHeadline_test, dfShortDescription_test, dfAuthors_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas de headline, short_description y authors\n",
    "df_train.drop(columns=['headline', 'short_description', 'authors'], inplace=True)\n",
    "df_test.drop(columns=['headline', 'short_description', 'authors'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder para \"date\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateLE = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = dateLE.fit_transform(df_train['date'])\n",
    "df_test['date'] = dateLE.fit_transform(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding para \"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_train['category'].to_numpy()\n",
    "categories = categories.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = categoryOHE.fit_transform(categories).toarray()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['category'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE ÁRBOL DE DECISIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De profundidad máxima 3, criterio Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos train en train y dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_tree, X_dev_tree, Y_train_tree, Y_devtree = train_test_split(\n",
    "    df_train.drop(['category'], axis=1),\n",
    "    y,\n",
    "    test_size=(0.1), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.fit(X_train_tree, Y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = arbol.predict(X_dev_tree)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred, Y_devtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 1 the highest element of an array and the rest to 0\n",
    "def set_to_one(array):\n",
    " newArr = np.zeros(array.shape)\n",
    " newArr[array.argmax()] = 1\n",
    " return newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0.1,0.2,0.3,0.8,0.5])\n",
    "set_to_one(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the matrix and set to 1 the highest element of each row\n",
    "def set_to_one_row(matrix):\n",
    "\tfor i in range(len(matrix)):\n",
    "\t\trow = matrix[i]\n",
    "\t\tmatrix[i] = set_to_one(row)\n",
    "\treturn matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_then_save(model, X_test, filename):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(Y_pred[0])\n",
    "    Y_pred = set_to_one_row(Y_pred)\n",
    "    Y_pred_decoded = categoryOHE.inverse_transform(Y_pred).astype(str).reshape(-1)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('id,category\\n')\n",
    "        for i, pred in enumerate(Y_pred_decoded):\n",
    "            f.write(str(i) + ',' + str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_then_save(arbol, df_test, 'submission_arbol.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NN_model(output_shapes, input_shape, use_bias, activations, dropouts, optimizer, loss, metric):\n",
    "    model = Sequential()\n",
    "    layers = len(output_shapes)\n",
    "    for i in range(0, layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(output_shapes[i], input_shape=(input_shape,), use_bias=use_bias))\n",
    "        else: \n",
    "            model.add(Dense(output_shapes[i], use_bias=use_bias))\n",
    "            model.add(Activation(activations[i]))\n",
    "        if dropouts[i] != None:\n",
    "            model.add(dropouts[i])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_NN_model(\n",
    "  output_shapes=[25, 35, Y_train_tree.shape[1]],\n",
    "  input_shape=len(df_train.drop(['category'], axis=1).columns),\n",
    "  use_bias=True,\n",
    "  activations=[LeakyReLU(alpha=0.3), LeakyReLU(alpha=0.3), 'softmax'],\n",
    "  dropouts=[Dropout(0.3), None, None],\n",
    "  optimizer='Adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metric='accuracy' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5)\n",
    "model.fit(X_train_tree, Y_train_tree, epochs=2000, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = model.predict(X_dev_tree)\n",
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = set_to_one_row(Y_pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred_model, Y_devtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a cargar los df para trabajar en copias limpias\n",
    "\n",
    "df_test = pd.read_csv(\"./new_classification_test.csv\", sep = \",\")\n",
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "def count_punctuation(text):\n",
    "\tcounter = Counter(char for char in text if char in punctuation)\n",
    "\tcounter_list = counter.values()\n",
    "\treturn reduce(lambda x, y: x + y, counter_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_punctuation('Hello, my name is John.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_features(df):\n",
    "  df['document_length'] = df['short_description'].map(lambda x: len(x))\n",
    "  df['author_count'] = df['authors'].map(lambda x: len(x.split(',')))\n",
    "  df['punctuation_count'] = df['short_description'].map(lambda x: count_punctuation(x))\n",
    "  return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_date(df):\n",
    "  dateLE = LabelEncoder()\n",
    "  df['date'] = dateLE.fit_transform(df['date'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_vectorization(_df):\n",
    "  _df = label_encode_date(_df)\n",
    "  \n",
    "  _headlines_corpus = list(_df['headline'])\n",
    "  _short_description_corpus = list(_df['short_description'])\n",
    "  _authors_corpus = list(_df['authors'])\n",
    "  \n",
    "  _headline_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(_headlines_corpus)\n",
    "  _short_description_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(_short_description_corpus)\n",
    "  _authors_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(_authors_corpus)\n",
    "  \n",
    "  _headlines = _headline_vectorizer.transform(_headlines_corpus)\n",
    "  _short_description = _short_description_vectorizer.transform(_short_description_corpus)\n",
    "  _authors = _authors_vectorizer.transform(_authors_corpus)\n",
    "  \n",
    "  _headline_feature_names = ['headline' + elem for elem in _headline_vectorizer.get_feature_names()]\n",
    "  _shortDescription_feature_names = ['shortDescription' + elem for elem in _short_description_vectorizer.get_feature_names()]\n",
    "  _authors_feature_names = ['authors' + elem for elem in _authors_vectorizer.get_feature_names()]\n",
    "  \n",
    "  _dfHeadline = pd.DataFrame.sparse.from_spmatrix(_headlines, columns=_headline_feature_names)\n",
    "  _dfShortDescription = pd.DataFrame.sparse.from_spmatrix(_short_description, columns=_shortDescription_feature_names)\n",
    "  _dfAuthors = pd.DataFrame.sparse.from_spmatrix(_authors, columns=_authors_feature_names)\n",
    "\n",
    "  _df.reset_index()\n",
    "\n",
    "  _df = pd.concat([_df, _dfHeadline, _dfShortDescription, _dfAuthors], axis=1)\n",
    "\n",
    "  _df.drop(['headline', 'short_description', 'authors'], axis=1, inplace=True)\n",
    "  return _df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, encoder):\n",
    "  categories = y.to_numpy()\n",
    "  categories = categories.reshape(-1,1)\n",
    "  return encoder.fit_transform(categories).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_categoryOHE = OneHotEncoder(handle_unknown='ignore')\n",
    "dev_categoryOHE = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "def do_attribute_engineering_with_split(df):\n",
    "\tdf['headline'].fillna('Unknown headline', inplace=True)\n",
    "\tdf['short_description'].fillna('Unknown short description', inplace=True)\n",
    "\tdf['authors'].fillna('Unknown author', inplace=True)\n",
    "\tdf[['headline', 'authors', 'short_description', 'date', 'category']] = df[['headline', 'authors', 'short_description', 'date', 'category']].astype('string')\n",
    "\tdf.drop(['id'], axis=1, inplace=True)\n",
    "\tdf = add_extra_features(df)\n",
    "\n",
    "\tX_train, X_dev, Y_train, Y_dev = train_test_split(\n",
    "    df.drop(['category'], axis=1),\n",
    "    df['category'],\n",
    "    test_size=(0.1), random_state=42)\n",
    "  \n",
    "\treturn perform_vectorization(X_train), perform_vectorization(X_dev), one_hot_encode(Y_train, train_categoryOHE), one_hot_encode(Y_dev, dev_categoryOHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_attribute_engineering_without_split(df):\n",
    "\tdf.drop(['id'], axis=1, inplace=True)\n",
    "\tdf['headline'].fillna('Unknown headline', inplace=True)\n",
    "\tdf['short_description'].fillna('Unknown short description', inplace=True)\n",
    "\tdf['authors'].fillna('Unknown author', inplace=True)\n",
    "\tdf[['headline', 'authors', 'short_description', 'date']] = df[['headline', 'authors', 'short_description', 'date']].astype('string')\n",
    "\tdf = add_extra_features(df)\n",
    "  \n",
    "\treturn perform_vectorization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "      <th>document_length</th>\n",
       "      <th>author_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>headline000</th>\n",
       "      <th>headline10</th>\n",
       "      <th>headline100</th>\n",
       "      <th>headline11</th>\n",
       "      <th>...</th>\n",
       "      <th>authorsyear</th>\n",
       "      <th>authorsyoga</th>\n",
       "      <th>authorsyork</th>\n",
       "      <th>authorsyoung</th>\n",
       "      <th>authorsyouth</th>\n",
       "      <th>authorszach</th>\n",
       "      <th>authorszeba</th>\n",
       "      <th>authorszogby</th>\n",
       "      <th>authorszuburbia</th>\n",
       "      <th>authorszupkus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2021</td>\n",
       "      <td>RELIGION</td>\n",
       "      <td>121</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>802</td>\n",
       "      <td>WEDDINGS</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>999</td>\n",
       "      <td>COMEDY</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1988</td>\n",
       "      <td>BUSINESS</td>\n",
       "      <td>82</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1761</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "      <td>39</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134566</th>\n",
       "      <td>134566</td>\n",
       "      <td>459</td>\n",
       "      <td>WELLNESS</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134567</th>\n",
       "      <td>134567</td>\n",
       "      <td>669</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "      <td>184</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134568</th>\n",
       "      <td>134568</td>\n",
       "      <td>5</td>\n",
       "      <td>PARENTING</td>\n",
       "      <td>199</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134569</th>\n",
       "      <td>134569</td>\n",
       "      <td>171</td>\n",
       "      <td>TRAVEL</td>\n",
       "      <td>133</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134570</th>\n",
       "      <td>134570</td>\n",
       "      <td>1637</td>\n",
       "      <td>STYLE</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134571 rows × 3006 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  date        category  document_length  author_count  \\\n",
       "0            0  2021        RELIGION              121             3   \n",
       "1            1   802        WEDDINGS              122             1   \n",
       "2            2   999          COMEDY               25             1   \n",
       "3            3  1988        BUSINESS               82             2   \n",
       "4            4  1761   ENTERTAINMENT               39             1   \n",
       "...        ...   ...             ...              ...           ...   \n",
       "134566  134566   459        WELLNESS              121             1   \n",
       "134567  134567   669  STYLE & BEAUTY              184             2   \n",
       "134568  134568     5       PARENTING              199             4   \n",
       "134569  134569   171          TRAVEL              133             1   \n",
       "134570  134570  1637           STYLE               28             1   \n",
       "\n",
       "        punctuation_count  headline000  headline10  headline100  headline11  \\\n",
       "0                       1            0           0            0           0   \n",
       "1                       3            0           0            0           0   \n",
       "2                       0            0           0            0           0   \n",
       "3                       2            0           1            0           0   \n",
       "4                       1            0           0            0           0   \n",
       "...                   ...          ...         ...          ...         ...   \n",
       "134566                  1            0           0            0           0   \n",
       "134567                  4            0           0            0           0   \n",
       "134568                  5            0           0            0           0   \n",
       "134569                  2            0           0            0           0   \n",
       "134570                  2            0           0            0           0   \n",
       "\n",
       "        ...  authorsyear  authorsyoga  authorsyork  authorsyoung  \\\n",
       "0       ...            0            0            0             0   \n",
       "1       ...            0            0            0             0   \n",
       "2       ...            0            0            0             0   \n",
       "3       ...            0            0            0             0   \n",
       "4       ...            0            0            0             0   \n",
       "...     ...          ...          ...          ...           ...   \n",
       "134566  ...            0            0            0             0   \n",
       "134567  ...            0            0            0             0   \n",
       "134568  ...            0            0            0             0   \n",
       "134569  ...            0            0            0             0   \n",
       "134570  ...            0            0            0             0   \n",
       "\n",
       "        authorsyouth  authorszach  authorszeba  authorszogby  authorszuburbia  \\\n",
       "0                  0            0            0             0                0   \n",
       "1                  0            0            0             0                0   \n",
       "2                  0            0            0             0                0   \n",
       "3                  0            0            0             0                0   \n",
       "4                  0            0            0             0                0   \n",
       "...              ...          ...          ...           ...              ...   \n",
       "134566             0            0            0             0                0   \n",
       "134567             0            0            0             0                0   \n",
       "134568             0            0            0             0                0   \n",
       "134569             0            0            0             0                0   \n",
       "134570             0            0            0             0                0   \n",
       "\n",
       "        authorszupkus  \n",
       "0                   0  \n",
       "1                   0  \n",
       "2                   0  \n",
       "3                   0  \n",
       "4                   0  \n",
       "...               ...  \n",
       "134566              0  \n",
       "134567              0  \n",
       "134568              0  \n",
       "134569              0  \n",
       "134570              0  \n",
       "\n",
       "[134571 rows x 3006 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")\n",
    "# X_train_nn_2, X_dev_nn_2, Y_train_nn_2, Y_dev_nn_2 = do_attribute_engineering_with_split(df_train)\n",
    "\n",
    "do_attribute_engineering_without_split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>document_length</th>\n",
       "      <th>author_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>headline000</th>\n",
       "      <th>headline10</th>\n",
       "      <th>headline100</th>\n",
       "      <th>headline11</th>\n",
       "      <th>headline12</th>\n",
       "      <th>headline13</th>\n",
       "      <th>...</th>\n",
       "      <th>authorsyear</th>\n",
       "      <th>authorsyoga</th>\n",
       "      <th>authorsyork</th>\n",
       "      <th>authorsyoung</th>\n",
       "      <th>authorsyouth</th>\n",
       "      <th>authorszach</th>\n",
       "      <th>authorszeba</th>\n",
       "      <th>authorszogby</th>\n",
       "      <th>authorszuburbia</th>\n",
       "      <th>authorszupkus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1761.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134565</th>\n",
       "      <td>632.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134567</th>\n",
       "      <td>669.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134568</th>\n",
       "      <td>5.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134569</th>\n",
       "      <td>171.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134570</th>\n",
       "      <td>1637.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133230 rows × 3004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  document_length  author_count  punctuation_count  headline000  \\\n",
       "0          NaN              NaN           NaN                NaN          0.0   \n",
       "1        802.0            122.0           1.0                3.0          0.0   \n",
       "2        999.0             25.0           1.0                0.0          0.0   \n",
       "3       1988.0             82.0           2.0                2.0          0.0   \n",
       "4       1761.0             39.0           1.0                1.0          0.0   \n",
       "...        ...              ...           ...                ...          ...   \n",
       "134565   632.0            189.0           3.0                3.0          NaN   \n",
       "134567   669.0            184.0           2.0                4.0          NaN   \n",
       "134568     5.0            199.0           4.0                5.0          NaN   \n",
       "134569   171.0            133.0           1.0                2.0          NaN   \n",
       "134570  1637.0             28.0           1.0                2.0          NaN   \n",
       "\n",
       "        headline10  headline100  headline11  headline12  headline13  ...  \\\n",
       "0              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "1              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "2              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "3              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "4              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "...            ...          ...         ...         ...         ...  ...   \n",
       "134565         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134567         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134568         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134569         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134570         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "\n",
       "        authorsyear  authorsyoga  authorsyork  authorsyoung  authorsyouth  \\\n",
       "0               0.0          0.0          0.0           0.0           0.0   \n",
       "1               0.0          0.0          0.0           0.0           0.0   \n",
       "2               0.0          0.0          0.0           0.0           0.0   \n",
       "3               0.0          0.0          0.0           0.0           0.0   \n",
       "4               0.0          0.0          0.0           0.0           0.0   \n",
       "...             ...          ...          ...           ...           ...   \n",
       "134565          NaN          NaN          NaN           NaN           NaN   \n",
       "134567          NaN          NaN          NaN           NaN           NaN   \n",
       "134568          NaN          NaN          NaN           NaN           NaN   \n",
       "134569          NaN          NaN          NaN           NaN           NaN   \n",
       "134570          NaN          NaN          NaN           NaN           NaN   \n",
       "\n",
       "        authorszach  authorszeba  authorszogby  authorszuburbia  authorszupkus  \n",
       "0               0.0          0.0           0.0              0.0            0.0  \n",
       "1               0.0          0.0           0.0              0.0            0.0  \n",
       "2               0.0          0.0           0.0              0.0            0.0  \n",
       "3               0.0          0.0           0.0              0.0            0.0  \n",
       "4               0.0          0.0           0.0              0.0            0.0  \n",
       "...             ...          ...           ...              ...            ...  \n",
       "134565          NaN          NaN           NaN              NaN            NaN  \n",
       "134567          NaN          NaN           NaN              NaN            NaN  \n",
       "134568          NaN          NaN           NaN              NaN            NaN  \n",
       "134569          NaN          NaN           NaN              NaN            NaN  \n",
       "134570          NaN          NaN           NaN              NaN            NaN  \n",
       "\n",
       "[133230 rows x 3004 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn_2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34086a273fcadbdf19105d1b4ac07fbb7b59a732fb2de6e1eab434b7949919a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('clasemlsi': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
