{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "\n",
    "df_test = pd.read_csv(\"./new_classification_test.csv\", sep = \",\")\n",
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de test\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de train\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los datos de train tienen una columna más respecto a los datos de test. Esta columna es la de la variable a predecir (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el dataset está formado de 134.571 observaciones. \n",
    "En total, hay 37.743 celdas sin datos, por lo que vamos a tener que tener esto en cuenta.\n",
    "\n",
    "La primera columna es un id, por lo que la podemos borrar, ya que no aporta nada de información, debido a que es un índice.\n",
    "\n",
    "La segunda columna, es el headline. Hay 133.726 datos distintos y 3 faltantes, por lo que podemos concluir que hay algunos headlines repetidos en el dataset.\n",
    "\n",
    "Algo similar ocurre con la columna authors. Solo el 20% de los datos son distintos, por lo que hay autores que se repiten. Esto puede ser útil porque tal vez un autor tenga preferencia a escribir noticias de una categoría en particular. Además, un dato no menor es que faltan 24.477 datos de autores. \n",
    "\n",
    "En cuanto a la categoria short_description, podemos observar que en su gran mayoría son distintos, pero hay algunas descripciones que se repiten. Además, también hay un gran número de valores faltantes (13.263).\n",
    "\n",
    "De la columna date podemos decir que hay muchos datos repetidos, ya que solo e 1.7% son valores distintos. Además, no falta ningún valor.\n",
    "\n",
    "Por último, la columna category, es la de la variable a predecir, Hay 41 categorías distintas en todo el set de datos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenamos datos faltantes:\n",
    "df_train['headline'].fillna('Unknown headline', inplace=True)\n",
    "df_test['headline'].fillna('Unknown headline', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['short_description'].fillna('Unknown short description', inplace=True)\n",
    "df_test['short_description'].fillna('Unknown short description', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['authors'].fillna('Unknown authors', inplace=True)\n",
    "df_test['authors'].fillna('Unknown authors', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columna de id\n",
    "# Quitamos la columna id del dataframe porque vimos que era un id autoincremental, el cual era lo mismo que el índice de la fila, por lo tanto, no aportaba información.\n",
    "df_train.drop(columns=['id'], axis=1, inplace=True)\n",
    "df_test.drop(columns=['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos el tipo de los datos a string, ya que antes eran de tipo object, pero en realidad, todos deberían ser strings\n",
    "df_train[['headline', 'authors', 'short_description', 'category', 'date']] = df_train[['headline', 'authors', 'short_description', 'category', 'date']].astype('string')\n",
    "df_test[['headline', 'authors', 'short_description', 'date']] = df_test[['headline', 'authors', 'short_description', 'date']].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 2309 fechas distintas en el dataset\n",
    "df_train['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 41 categorias distintas en el dataset\n",
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "df_train['category'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countplot por categoria\n",
    "plt.figure(figsize=(50,5))\n",
    "sns.countplot(data=df_train, x='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la mayoría de noticias son de la categoría política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma nos da una noción global de como es la distribución de \"category\". Los valores más frecuentes son:\n",
    "\n",
    "1 - Politics (21935)\n",
    "\n",
    "2 - Wellness (11944)\n",
    "\n",
    "3 - Entertainment (10759)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_corpus = list(df_train['headline'])\n",
    "headlines_corpus_test = list(df_test['headline'])\n",
    "\n",
    "headline_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus)\n",
    "headline_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headline_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headlines = headline_vectorizer.transform(headlines_corpus)\n",
    "X_test_headlines = headline_vectorizer_test.transform(headlines_corpus_test)\n",
    "print(X_train_headlines.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_description_corpus = list(df_train['short_description'])\n",
    "short_description_corpus_test = list(df_test['short_description'])\n",
    "\n",
    "shortDescription_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus)\n",
    "shortDescription_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortDescription_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shortDescription_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short_description = shortDescription_vectorizer.transform(short_description_corpus)\n",
    "X_test_short_description = shortDescription_vectorizer_test.transform(short_description_corpus_test)\n",
    "print(X_train_short_description.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_corpus = list(df_train['authors'])\n",
    "authors_corpus_test = list(df_test['authors'])\n",
    "\n",
    "authors_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus)\n",
    "authors_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_authors = authors_vectorizer.transform(authors_corpus)\n",
    "X_test_authors = authors_vectorizer_test.transform(authors_corpus_test)\n",
    "print(X_train_authors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora concatenamos los vectores de headlines, short_description y authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "headline_feature_names = ['headline' + elem for elem in headline_vectorizer.get_feature_names()]\n",
    "shortDescription_feature_names = ['shortDescription' + elem for elem in shortDescription_vectorizer.get_feature_names()]\n",
    "authors_feature_names = ['authors' + elem for elem in authors_vectorizer.get_feature_names()]\n",
    "dfHeadline = pd.DataFrame.sparse.from_spmatrix(X_train_headlines, columns=headline_feature_names)\n",
    "dfShortDescription = pd.DataFrame.sparse.from_spmatrix(X_train_short_description, columns=shortDescription_feature_names)\n",
    "dfAuthors = pd.DataFrame.sparse.from_spmatrix(X_train_authors, columns=authors_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "headline_feature_names_test = ['headline' + elem for elem in headline_vectorizer_test.get_feature_names()]\n",
    "shortDescription_feature_names_test = ['shortDescription' + elem for elem in shortDescription_vectorizer_test.get_feature_names()]\n",
    "authors_feature_names_test = ['authors' + elem for elem in authors_vectorizer_test.get_feature_names()]\n",
    "dfHeadline_test = pd.DataFrame.sparse.from_spmatrix(X_test_headlines, columns=headline_feature_names_test)\n",
    "dfShortDescription_test = pd.DataFrame.sparse.from_spmatrix(X_test_short_description, columns=shortDescription_feature_names_test)\n",
    "dfAuthors_test = pd.DataFrame.sparse.from_spmatrix(X_test_authors, columns=authors_feature_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAuthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, dfHeadline, dfShortDescription, dfAuthors], axis=1)\n",
    "df_test = pd.concat([df_test, dfHeadline_test, dfShortDescription_test, dfAuthors_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas de headline, short_description y authors\n",
    "df_train.drop(columns=['headline', 'short_description', 'authors'], inplace=True)\n",
    "df_test.drop(columns=['headline', 'short_description', 'authors'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder para \"date\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateLE = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = dateLE.fit_transform(df_train['date'])\n",
    "df_test['date'] = dateLE.fit_transform(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding para \"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_train['category'].to_numpy()\n",
    "categories = categories.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = categoryOHE.fit_transform(categories).toarray()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['category'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE ÁRBOL DE DECISIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De profundidad máxima 3, criterio Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos train en train y dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_tree, X_dev_tree, Y_train_tree, Y_devtree = train_test_split(\n",
    "    df_train.drop(['category'], axis=1),\n",
    "    y,\n",
    "    test_size=(0.1), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.fit(X_train_tree, Y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = arbol.predict(X_dev_tree)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred, Y_devtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 1 the highest element of an array and the rest to 0\n",
    "def set_to_one(array):\n",
    " newArr = np.zeros(array.shape)\n",
    " newArr[array.argmax()] = 1\n",
    " return newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0.1,0.2,0.3,0.8,0.5])\n",
    "set_to_one(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the matrix and set to 1 the highest element of each row\n",
    "def set_to_one_row(matrix):\n",
    "\tfor i in range(len(matrix)):\n",
    "\t\trow = matrix[i]\n",
    "\t\tmatrix[i] = set_to_one(row)\n",
    "\treturn matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_then_save(model, X_test, filename):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(Y_pred[0])\n",
    "    Y_pred = set_to_one_row(Y_pred)\n",
    "    Y_pred_decoded = categoryOHE.inverse_transform(Y_pred).astype(str).reshape(-1)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('id,category\\n')\n",
    "        for i, pred in enumerate(Y_pred_decoded):\n",
    "            f.write(str(i) + ',' + str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_then_save(arbol, df_test, 'submission_arbol.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NN_model(output_shapes, input_shape, use_bias, activations, dropouts, optimizer, loss, metric):\n",
    "    model = Sequential()\n",
    "    layers = len(output_shapes)\n",
    "    for i in range(0, layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(output_shapes[i], input_shape=(input_shape,), use_bias=use_bias))\n",
    "        else: \n",
    "            model.add(Dense(output_shapes[i], use_bias=use_bias))\n",
    "            model.add(Activation(activations[i]))\n",
    "        if dropouts[i] != None:\n",
    "            model.add(dropouts[i])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_NN_model(\n",
    "  output_shapes=[25, 35, Y_train_tree.shape[1]],\n",
    "  input_shape=len(df_train.drop(['category'], axis=1).columns),\n",
    "  use_bias=True,\n",
    "  activations=[LeakyReLU(alpha=0.3), LeakyReLU(alpha=0.3), 'softmax'],\n",
    "  dropouts=[Dropout(0.3), None, None],\n",
    "  optimizer='Adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metric='accuracy' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5)\n",
    "model.fit(X_train_tree, Y_train_tree, epochs=2000, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = model.predict(X_dev_tree)\n",
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = set_to_one_row(Y_pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred_model, Y_devtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Volvemos a cargar los df para trabajar en copias limpias\n",
    "\n",
    "df_test = pd.read_csv(\"./new_classification_test.csv\", sep = \",\")\n",
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "from collections import Counter\n",
    "from functools import reduce\n",
    "\n",
    "def count_punctuation(text):\n",
    "\tcounter = Counter(char for char in text if char in punctuation)\n",
    "\tcounter_list = counter.values()\n",
    "\treturn reduce(lambda x, y: x + y, counter_list, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_punctuation('Hello, my name is John.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10912</th>\n",
       "      <td>10912</td>\n",
       "      <td>About 3% Of Americans Sick With The Flu, Surve...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Authors of the report noted that the findings ...</td>\n",
       "      <td>2014-01-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59139</th>\n",
       "      <td>59139</td>\n",
       "      <td>Recipe Of The Day: Croissant Pudding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>And croissants play a major role.</td>\n",
       "      <td>2012-11-16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68553</th>\n",
       "      <td>68553</td>\n",
       "      <td>So You're Newly Engaged... Now What?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Enjoy the Ride The planning is exciting, but c...</td>\n",
       "      <td>2014-01-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108983</th>\n",
       "      <td>108983</td>\n",
       "      <td>Even Nonsmoking Teens Are Constantly Exposed T...</td>\n",
       "      <td>Lindsey Tanner, Associated Press</td>\n",
       "      <td>There's no safe level of secondhand smoke expo...</td>\n",
       "      <td>2016-01-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3912</th>\n",
       "      <td>3912</td>\n",
       "      <td>Jeb and Hillary Baggage Wars</td>\n",
       "      <td>Phil Perrier, ContributorFreelance Writer and ...</td>\n",
       "      <td>The conventional wisdom at the moment is that ...</td>\n",
       "      <td>2015-02-23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110268</th>\n",
       "      <td>110268</td>\n",
       "      <td>Wedding Trends 2012: Industry Insiders Choose ...</td>\n",
       "      <td>Stephanie Hallett</td>\n",
       "      <td>What a year it's been for wedding enthusiasts....</td>\n",
       "      <td>2012-12-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>119879</td>\n",
       "      <td>Donald Trump Now Says Humans Somehow Contribut...</td>\n",
       "      <td>Laura Barrón-López</td>\n",
       "      <td>\"There is some connectivity.\"</td>\n",
       "      <td>2016-11-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103694</th>\n",
       "      <td>103694</td>\n",
       "      <td>Rene Gonzalez, Architect, Designs Stunning Bui...</td>\n",
       "      <td>Brie Dyas</td>\n",
       "      <td>Here, every room has a great view.</td>\n",
       "      <td>2013-07-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>131932</td>\n",
       "      <td>10 Steps to 'Living in the Moment'</td>\n",
       "      <td>Rea Nolan Martin, Contributor\\nAward-winning a...</td>\n",
       "      <td>In a nutshell, to live in the moment means to ...</td>\n",
       "      <td>2014-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>121958</td>\n",
       "      <td>The Ebola Plague: African Heroes and Martyrs</td>\n",
       "      <td>Kwei Quartey, ContributorForeign Policy In Foc...</td>\n",
       "      <td>During the height of the Ebola panic, nurse Ka...</td>\n",
       "      <td>2015-03-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>121113 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           headline  \\\n",
       "10912    10912  About 3% Of Americans Sick With The Flu, Surve...   \n",
       "59139    59139               Recipe Of The Day: Croissant Pudding   \n",
       "68553    68553               So You're Newly Engaged... Now What?   \n",
       "108983  108983  Even Nonsmoking Teens Are Constantly Exposed T...   \n",
       "3912      3912                       Jeb and Hillary Baggage Wars   \n",
       "...        ...                                                ...   \n",
       "110268  110268  Wedding Trends 2012: Industry Insiders Choose ...   \n",
       "119879  119879  Donald Trump Now Says Humans Somehow Contribut...   \n",
       "103694  103694  Rene Gonzalez, Architect, Designs Stunning Bui...   \n",
       "131932  131932                 10 Steps to 'Living in the Moment'   \n",
       "121958  121958       The Ebola Plague: African Heroes and Martyrs   \n",
       "\n",
       "                                                  authors  \\\n",
       "10912                                                 NaN   \n",
       "59139                                                 NaN   \n",
       "68553                                                 NaN   \n",
       "108983                   Lindsey Tanner, Associated Press   \n",
       "3912    Phil Perrier, ContributorFreelance Writer and ...   \n",
       "...                                                   ...   \n",
       "110268                                  Stephanie Hallett   \n",
       "119879                                 Laura Barrón-López   \n",
       "103694                                          Brie Dyas   \n",
       "131932  Rea Nolan Martin, Contributor\\nAward-winning a...   \n",
       "121958  Kwei Quartey, ContributorForeign Policy In Foc...   \n",
       "\n",
       "                                        short_description        date  \n",
       "10912   Authors of the report noted that the findings ...  2014-01-10  \n",
       "59139                   And croissants play a major role.  2012-11-16  \n",
       "68553   Enjoy the Ride The planning is exciting, but c...  2014-01-12  \n",
       "108983  There's no safe level of secondhand smoke expo...  2016-01-11  \n",
       "3912    The conventional wisdom at the moment is that ...  2015-02-23  \n",
       "...                                                   ...         ...  \n",
       "110268  What a year it's been for wedding enthusiasts....  2012-12-29  \n",
       "119879                      \"There is some connectivity.\"  2016-11-22  \n",
       "103694                 Here, every room has a great view.  2013-07-12  \n",
       "131932  In a nutshell, to live in the moment means to ...  2014-03-22  \n",
       "121958  During the height of the Ebola panic, nurse Ka...  2015-03-04  \n",
       "\n",
       "[121113 rows x 5 columns]"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_dev, Y_train, Y_dev = train_test_split(\n",
    "    df_train.drop(['category'], axis=1),\n",
    "    df_train['category'],\n",
    "    test_size=(0.1), random_state=42)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>headline</th>\n",
       "      <th>authors</th>\n",
       "      <th>short_description</th>\n",
       "      <th>date</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Does Romans 13 Give The President The Right To...</td>\n",
       "      <td>David J. Dunn, PhD, ContributorTheological tro...</td>\n",
       "      <td>Let any who doubt the lamentable state of theo...</td>\n",
       "      <td>2017-08-12</td>\n",
       "      <td>RELIGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How To Throw A Marijuana-Friendly Wedding</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Designate a pot-smoking area. While brides and...</td>\n",
       "      <td>2014-04-09</td>\n",
       "      <td>WEDDINGS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>The New Yorker Finally Realized Us Plebeians D...</td>\n",
       "      <td>Katla McGlynn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2014-10-23</td>\n",
       "      <td>COMEDY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>The 10 Best U.S. Cities For Retirement</td>\n",
       "      <td>PureWow, ContributorWhat your girl crush reads...</td>\n",
       "      <td>Next question: When is too early to start coun...</td>\n",
       "      <td>2017-07-10</td>\n",
       "      <td>BUSINESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Justin Bieber Punches Man In The Face, Leaves ...</td>\n",
       "      <td>Carly Ledbetter</td>\n",
       "      <td>This fan got WAY too close for comfort.</td>\n",
       "      <td>2016-11-23</td>\n",
       "      <td>ENTERTAINMENT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134566</th>\n",
       "      <td>134566</td>\n",
       "      <td>Added Sugars Make Up 13 Percent Of U.S. Adults...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A recent study of added sugar consumption in k...</td>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>WELLNESS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134567</th>\n",
       "      <td>134567</td>\n",
       "      <td>Must-Try Bohemian Trends</td>\n",
       "      <td>W Magazine, Contributor\\nW Magazine</td>\n",
       "      <td>Whether it was the suede dresses at Calvin Kle...</td>\n",
       "      <td>2013-11-27</td>\n",
       "      <td>STYLE &amp; BEAUTY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134568</th>\n",
       "      <td>134568</td>\n",
       "      <td>From A Dad Who Supports Breastfeeding: My Boys...</td>\n",
       "      <td>John Willey, Contributor\\nBlogger, DaddysInCha...</td>\n",
       "      <td>What is this world coming to that people would...</td>\n",
       "      <td>2012-02-02</td>\n",
       "      <td>PARENTING</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134569</th>\n",
       "      <td>134569</td>\n",
       "      <td>Bikinis, Texas: Businessman Buys Bankersmith, ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The town was first established in 1913, when t...</td>\n",
       "      <td>2012-07-17</td>\n",
       "      <td>TRAVEL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134570</th>\n",
       "      <td>134570</td>\n",
       "      <td>The Fashion At The RNC Was Almost As Over The ...</td>\n",
       "      <td>Damon Dahlen and Lee Moran</td>\n",
       "      <td>A Donald Trump cape, anyone?</td>\n",
       "      <td>2016-07-22</td>\n",
       "      <td>STYLE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134571 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id                                           headline  \\\n",
       "0            0  Does Romans 13 Give The President The Right To...   \n",
       "1            1          How To Throw A Marijuana-Friendly Wedding   \n",
       "2            2  The New Yorker Finally Realized Us Plebeians D...   \n",
       "3            3             The 10 Best U.S. Cities For Retirement   \n",
       "4            4  Justin Bieber Punches Man In The Face, Leaves ...   \n",
       "...        ...                                                ...   \n",
       "134566  134566  Added Sugars Make Up 13 Percent Of U.S. Adults...   \n",
       "134567  134567                           Must-Try Bohemian Trends   \n",
       "134568  134568  From A Dad Who Supports Breastfeeding: My Boys...   \n",
       "134569  134569  Bikinis, Texas: Businessman Buys Bankersmith, ...   \n",
       "134570  134570  The Fashion At The RNC Was Almost As Over The ...   \n",
       "\n",
       "                                                  authors  \\\n",
       "0       David J. Dunn, PhD, ContributorTheological tro...   \n",
       "1                                                     NaN   \n",
       "2                                           Katla McGlynn   \n",
       "3       PureWow, ContributorWhat your girl crush reads...   \n",
       "4                                         Carly Ledbetter   \n",
       "...                                                   ...   \n",
       "134566                                                NaN   \n",
       "134567                W Magazine, Contributor\\nW Magazine   \n",
       "134568  John Willey, Contributor\\nBlogger, DaddysInCha...   \n",
       "134569                                                NaN   \n",
       "134570                         Damon Dahlen and Lee Moran   \n",
       "\n",
       "                                        short_description        date  \\\n",
       "0       Let any who doubt the lamentable state of theo...  2017-08-12   \n",
       "1       Designate a pot-smoking area. While brides and...  2014-04-09   \n",
       "2                                                     NaN  2014-10-23   \n",
       "3       Next question: When is too early to start coun...  2017-07-10   \n",
       "4                 This fan got WAY too close for comfort.  2016-11-23   \n",
       "...                                                   ...         ...   \n",
       "134566  A recent study of added sugar consumption in k...  2013-05-01   \n",
       "134567  Whether it was the suede dresses at Calvin Kle...  2013-11-27   \n",
       "134568  What is this world coming to that people would...  2012-02-02   \n",
       "134569  The town was first established in 1913, when t...  2012-07-17   \n",
       "134570                       A Donald Trump cape, anyone?  2016-07-22   \n",
       "\n",
       "              category  \n",
       "0             RELIGION  \n",
       "1             WEDDINGS  \n",
       "2               COMEDY  \n",
       "3             BUSINESS  \n",
       "4        ENTERTAINMENT  \n",
       "...                ...  \n",
       "134566        WELLNESS  \n",
       "134567  STYLE & BEAUTY  \n",
       "134568       PARENTING  \n",
       "134569          TRAVEL  \n",
       "134570           STYLE  \n",
       "\n",
       "[134571 rows x 6 columns]"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_extra_features(df):\n",
    "  df['document_length'] = df['short_description'].map(lambda x: len(x))\n",
    "  df['author_count'] = df['authors'].map(lambda x: len(x.split(',')))\n",
    "  df['punctuation_count'] = df['short_description'].map(lambda x: count_punctuation(x))\n",
    "  return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_date(df):\n",
    "  dateLE = LabelEncoder()\n",
    "  df['date'] = dateLE.fit_transform(df['date'])\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_vectorization(df):\n",
    "  df = label_encode_date(df)\n",
    "  \n",
    "  headlines_corpus = list(df['headline'])\n",
    "  short_description_corpus = list(df['short_description'])\n",
    "  authors_corpus = list(df['authors'])\n",
    "  \n",
    "  headline_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus)\n",
    "  short_description_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(short_description_corpus)\n",
    "  authors_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus)\n",
    "  \n",
    "  _headlines = headline_vectorizer.transform(headlines_corpus)\n",
    "  _short_description = short_description_vectorizer.transform(short_description_corpus)\n",
    "  _authors = authors_vectorizer.transform(authors_corpus)\n",
    "  \n",
    "  headline_feature_names = ['headline' + elem for elem in headline_vectorizer.get_feature_names()]\n",
    "  shortDescription_feature_names = ['shortDescription' + elem for elem in shortDescription_vectorizer.get_feature_names()]\n",
    "  authors_feature_names = ['authors' + elem for elem in authors_vectorizer.get_feature_names()]\n",
    "  \n",
    "  dfHeadline = pd.DataFrame.sparse.from_spmatrix(_headlines, columns=headline_feature_names)\n",
    "  dfShortDescription = pd.DataFrame.sparse.from_spmatrix(_short_description, columns=shortDescription_feature_names)\n",
    "  dfAuthors = pd.DataFrame.sparse.from_spmatrix(_authors, columns=authors_feature_names)\n",
    "\n",
    "  print(dfShortDescription)\n",
    "\n",
    "  df = pd.concat([df, dfHeadline, dfShortDescription, dfAuthors], axis=1)\n",
    "\n",
    "  df.drop(['headline', 'short_description', 'authors'], axis=1, inplace=True)\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(y, encoder):\n",
    "  categories = y.to_numpy()\n",
    "  categories = categories.reshape(-1,1)\n",
    "  return categoryOHE.fit_transform(categories).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_categoryOHE = OneHotEncoder(handle_unknown='ignore')\n",
    "dev_categoryOHE = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "def do_attribute_engineering_with_split(df):\n",
    "\tdf['headline'].fillna('Unknown headline', inplace=True)\n",
    "\tdf['short_description'].fillna('Unknown short description', inplace=True)\n",
    "\tdf['authors'].fillna('Unknown author', inplace=True)\n",
    "\tdf[['headline', 'authors', 'short_description', 'date', 'category']] = df[['headline', 'authors', 'short_description', 'date', 'category']].astype('string')\n",
    "\tdf.drop(['id'], axis=1, inplace=True)\n",
    "\tdf = add_extra_features(df)\n",
    "\n",
    "\tX_train, X_dev, Y_train, Y_dev = train_test_split(\n",
    "    df.drop(['category'], axis=1),\n",
    "    df['category'],\n",
    "    test_size=(0.1), random_state=42)\n",
    "  \n",
    "\treturn perform_vectorization(X_train), perform_vectorization(X_dev), one_hot_encode(Y_train, train_categoryOHE), one_hot_encode(Y_dev, dev_categoryOHE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_attribute_engineering_without_split(df):\n",
    "\tdf['headline'].fillna('Unknown headline', inplace=True)\n",
    "\tdf['short_description'].fillna('Unknown short description', inplace=True)\n",
    "\tdf['authors'].fillna('Unknown author', inplace=True)\n",
    "\tdf[['headline', 'authors', 'short_description', 'date']] = df[['headline', 'authors', 'short_description', 'date']].astype('string')\n",
    "\tdf = add_extra_features(df)\n",
    "  \n",
    "\treturn perform_vectorization(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_punctuation(df_train['short_description'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         1\n",
       "1         3\n",
       "2         0\n",
       "3         2\n",
       "4         1\n",
       "         ..\n",
       "134566    1\n",
       "134567    4\n",
       "134568    5\n",
       "134569    2\n",
       "134570    2\n",
       "Name: short_description, Length: 134571, dtype: int64"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train['short_description'].map(lambda x: count_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        shortDescription000  shortDescription10  shortDescription100  \\\n",
      "0                         0                   0                    0   \n",
      "1                         0                   0                    0   \n",
      "2                         0                   0                    0   \n",
      "3                         0                   0                    0   \n",
      "4                         0                   0                    0   \n",
      "...                     ...                 ...                  ...   \n",
      "121108                    0                   0                    0   \n",
      "121109                    0                   0                    0   \n",
      "121110                    0                   0                    0   \n",
      "121111                    0                   0                    0   \n",
      "121112                    0                   0                    0   \n",
      "\n",
      "        shortDescription11  shortDescription12  shortDescription13  \\\n",
      "0                        0                   0                   0   \n",
      "1                        0                   0                   0   \n",
      "2                        0                   0                   0   \n",
      "3                        0                   0                   0   \n",
      "4                        0                   0                   0   \n",
      "...                    ...                 ...                 ...   \n",
      "121108                   0                   0                   0   \n",
      "121109                   0                   0                   0   \n",
      "121110                   0                   0                   0   \n",
      "121111                   0                   0                   0   \n",
      "121112                   0                   0                   0   \n",
      "\n",
      "        shortDescription14  shortDescription15  shortDescription18  \\\n",
      "0                        0                   0                   0   \n",
      "1                        0                   0                   0   \n",
      "2                        0                   0                   0   \n",
      "3                        0                   0                   0   \n",
      "4                        0                   0                   0   \n",
      "...                    ...                 ...                 ...   \n",
      "121108                   0                   0                   0   \n",
      "121109                   0                   0                   0   \n",
      "121110                   0                   0                   0   \n",
      "121111                   0                   0                   0   \n",
      "121112                   0                   0                   0   \n",
      "\n",
      "        shortDescription20  ...  shortDescriptionwriting  \\\n",
      "0                        0  ...                        0   \n",
      "1                        0  ...                        0   \n",
      "2                        0  ...                        0   \n",
      "3                        0  ...                        0   \n",
      "4                        0  ...                        0   \n",
      "...                    ...  ...                      ...   \n",
      "121108                   0  ...                        0   \n",
      "121109                   0  ...                        0   \n",
      "121110                   0  ...                        0   \n",
      "121111                   0  ...                        0   \n",
      "121112                   0  ...                        0   \n",
      "\n",
      "        shortDescriptionwritten  shortDescriptionwrong  shortDescriptionwrote  \\\n",
      "0                             0                      0                      0   \n",
      "1                             0                      0                      0   \n",
      "2                             0                      0                      0   \n",
      "3                             0                      0                      0   \n",
      "4                             0                      1                      0   \n",
      "...                         ...                    ...                    ...   \n",
      "121108                        0                      0                      0   \n",
      "121109                        0                      0                      0   \n",
      "121110                        0                      0                      0   \n",
      "121111                        0                      0                      0   \n",
      "121112                        0                      0                      0   \n",
      "\n",
      "        shortDescriptionyear  shortDescriptionyears  shortDescriptionyes  \\\n",
      "0                          0                      0                    0   \n",
      "1                          0                      0                    0   \n",
      "2                          0                      0                    0   \n",
      "3                          0                      0                    0   \n",
      "4                          0                      0                    0   \n",
      "...                      ...                    ...                  ...   \n",
      "121108                     1                      0                    0   \n",
      "121109                     0                      0                    0   \n",
      "121110                     0                      0                    0   \n",
      "121111                     0                      0                    0   \n",
      "121112                     0                      0                    0   \n",
      "\n",
      "        shortDescriptionyoga  shortDescriptionyork  shortDescriptionyoung  \n",
      "0                          0                     0                      0  \n",
      "1                          0                     0                      0  \n",
      "2                          0                     0                      0  \n",
      "3                          0                     0                      0  \n",
      "4                          0                     0                      0  \n",
      "...                      ...                   ...                    ...  \n",
      "121108                     0                     0                      0  \n",
      "121109                     0                     0                      0  \n",
      "121110                     0                     0                      0  \n",
      "121111                     0                     0                      0  \n",
      "121112                     0                     0                      0  \n",
      "\n",
      "[121113 rows x 1000 columns]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-228-a47acc5fe1d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdf_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./new_classification_train.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mX_train_nn_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_dev_nn_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train_nn_2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_dev_nn_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdo_attribute_engineering_with_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-206-7d6f0bf13c6d>\u001b[0m in \u001b[0;36mdo_attribute_engineering_with_split\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     15\u001b[0m     test_size=(0.1), random_state=42)\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mperform_vectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mperform_vectorization\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_dev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_categoryOHE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mone_hot_encode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_dev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdev_categoryOHE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-227-f802858af720>\u001b[0m in \u001b[0;36mperform_vectorization\u001b[1;34m(df)\u001b[0m\n\u001b[0;32m     24\u001b[0m   \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdfShortDescription\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m   \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfHeadline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfShortDescription\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdfAuthors\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m   \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'headline'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'short_description'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'authors'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    519\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 520\u001b[1;33m             new_data = concatenate_block_managers(\n\u001b[0m\u001b[0;32m    521\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnew_axes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbm_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    522\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mconcatenate_block_managers\u001b[1;34m(mgrs_indexers, axes, concat_axis, copy)\u001b[0m\n\u001b[0;32m     81\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m             b = make_block(\n\u001b[1;32m---> 83\u001b[1;33m                 \u001b[0m_concatenate_join_units\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcat_axis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m                 \u001b[0mplacement\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m                 \u001b[0mndim\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m_concatenate_join_units\u001b[1;34m(join_units, concat_axis, copy)\u001b[0m\n\u001b[0;32m    311\u001b[0m     \u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_empty_dtype_and_na\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjoin_units\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 313\u001b[1;33m     to_concat = [\n\u001b[0m\u001b[0;32m    314\u001b[0m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m     to_concat = [\n\u001b[1;32m--> 314\u001b[1;33m         \u001b[0mju\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_reindexed_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mempty_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupcasted_na\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupcasted_na\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    315\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mju\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mjoin_units\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m     ]\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\internals\\concat.py\u001b[0m in \u001b[0;36mget_reindexed_values\u001b[1;34m(self, empty_dtype, upcasted_na)\u001b[0m\n\u001b[0;32m    296\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m                 \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0malgos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake_nd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[1;34m(arr, indexer, axis, out, fill_value, allow_fill)\u001b[0m\n\u001b[0;32m   1697\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mABCExtensionArray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1698\u001b[0m         \u001b[1;31m# Check for EA to catch DatetimeArray, TimedeltaArray\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1699\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1700\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1701\u001b[0m     \u001b[0marr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mextract_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m    829\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"dtype\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    830\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 831\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_take_with_fill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, data, sparse_index, index, fill_value, kind, dtype, copy)\u001b[0m\n\u001b[0;32m    383\u001b[0m                     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"datetime64[ns]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m             sparse_values, sparse_index, fill_value = make_sparse(\n\u001b[0m\u001b[0;32m    386\u001b[0m                 \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkind\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\sparse\\array.py\u001b[0m in \u001b[0;36mmake_sparse\u001b[1;34m(arr, kind, fill_value, dtype)\u001b[0m\n\u001b[0;32m   1500\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_mask_object_ndarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1501\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1502\u001b[1;33m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1503\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1504\u001b[0m     \u001b[0mlength\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")\n",
    "X_train_nn_2, X_dev_nn_2, Y_train_nn_2, Y_dev_nn_2 = do_attribute_engineering_with_split(df_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>date</th>\n",
       "      <th>document_length</th>\n",
       "      <th>author_count</th>\n",
       "      <th>punctuation_count</th>\n",
       "      <th>headline000</th>\n",
       "      <th>headline10</th>\n",
       "      <th>headline100</th>\n",
       "      <th>headline11</th>\n",
       "      <th>headline12</th>\n",
       "      <th>headline13</th>\n",
       "      <th>...</th>\n",
       "      <th>authorsyear</th>\n",
       "      <th>authorsyoga</th>\n",
       "      <th>authorsyork</th>\n",
       "      <th>authorsyoung</th>\n",
       "      <th>authorsyouth</th>\n",
       "      <th>authorszach</th>\n",
       "      <th>authorszeba</th>\n",
       "      <th>authorszogby</th>\n",
       "      <th>authorszuburbia</th>\n",
       "      <th>authorszupkus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>802.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>999.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1988.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1761.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134565</th>\n",
       "      <td>632.0</td>\n",
       "      <td>189.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134567</th>\n",
       "      <td>669.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134568</th>\n",
       "      <td>5.0</td>\n",
       "      <td>199.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134569</th>\n",
       "      <td>171.0</td>\n",
       "      <td>133.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134570</th>\n",
       "      <td>1637.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>133230 rows × 3004 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          date  document_length  author_count  punctuation_count  headline000  \\\n",
       "0          NaN              NaN           NaN                NaN          0.0   \n",
       "1        802.0            122.0           1.0                3.0          0.0   \n",
       "2        999.0             25.0           1.0                0.0          0.0   \n",
       "3       1988.0             82.0           2.0                2.0          0.0   \n",
       "4       1761.0             39.0           1.0                1.0          0.0   \n",
       "...        ...              ...           ...                ...          ...   \n",
       "134565   632.0            189.0           3.0                3.0          NaN   \n",
       "134567   669.0            184.0           2.0                4.0          NaN   \n",
       "134568     5.0            199.0           4.0                5.0          NaN   \n",
       "134569   171.0            133.0           1.0                2.0          NaN   \n",
       "134570  1637.0             28.0           1.0                2.0          NaN   \n",
       "\n",
       "        headline10  headline100  headline11  headline12  headline13  ...  \\\n",
       "0              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "1              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "2              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "3              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "4              0.0          0.0         0.0         0.0         0.0  ...   \n",
       "...            ...          ...         ...         ...         ...  ...   \n",
       "134565         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134567         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134568         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134569         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "134570         NaN          NaN         NaN         NaN         NaN  ...   \n",
       "\n",
       "        authorsyear  authorsyoga  authorsyork  authorsyoung  authorsyouth  \\\n",
       "0               0.0          0.0          0.0           0.0           0.0   \n",
       "1               0.0          0.0          0.0           0.0           0.0   \n",
       "2               0.0          0.0          0.0           0.0           0.0   \n",
       "3               0.0          0.0          0.0           0.0           0.0   \n",
       "4               0.0          0.0          0.0           0.0           0.0   \n",
       "...             ...          ...          ...           ...           ...   \n",
       "134565          NaN          NaN          NaN           NaN           NaN   \n",
       "134567          NaN          NaN          NaN           NaN           NaN   \n",
       "134568          NaN          NaN          NaN           NaN           NaN   \n",
       "134569          NaN          NaN          NaN           NaN           NaN   \n",
       "134570          NaN          NaN          NaN           NaN           NaN   \n",
       "\n",
       "        authorszach  authorszeba  authorszogby  authorszuburbia  authorszupkus  \n",
       "0               0.0          0.0           0.0              0.0            0.0  \n",
       "1               0.0          0.0           0.0              0.0            0.0  \n",
       "2               0.0          0.0           0.0              0.0            0.0  \n",
       "3               0.0          0.0           0.0              0.0            0.0  \n",
       "4               0.0          0.0           0.0              0.0            0.0  \n",
       "...             ...          ...           ...              ...            ...  \n",
       "134565          NaN          NaN           NaN              NaN            NaN  \n",
       "134567          NaN          NaN           NaN              NaN            NaN  \n",
       "134568          NaN          NaN           NaN              NaN            NaN  \n",
       "134569          NaN          NaN           NaN              NaN            NaN  \n",
       "134570          NaN          NaN           NaN              NaN            NaN  \n",
       "\n",
       "[133230 rows x 3004 columns]"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_nn_2"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34086a273fcadbdf19105d1b4ac07fbb7b59a732fb2de6e1eab434b7949919a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('clasemlsi': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
