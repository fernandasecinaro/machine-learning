{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carga de los datos\n",
    "\n",
    "df_test = pd.read_csv(\"./new_classification_test.csv\", sep = \",\")\n",
    "df_train = pd.read_csv(\"./new_classification_train.csv\", sep = \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de test\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mostramos dataset de train\n",
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los datos de train tienen una columna más respecto a los datos de test. Esta columna es la de la variable a predecir (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, el dataset está formado de 134.571 observaciones. \n",
    "En total, hay 37.743 celdas sin datos, por lo que vamos a tener que tener esto en cuenta.\n",
    "\n",
    "La primera columna es un id, por lo que la podemos borrar, ya que no aporta nada de información, debido a que es un índice.\n",
    "\n",
    "La segunda columna, es el headline. Hay 133.726 datos distintos y 3 faltantes, por lo que podemos concluir que hay algunos headlines repetidos en el dataset.\n",
    "\n",
    "Algo similar ocurre con la columna authors. Solo el 20% de los datos son distintos, por lo que hay autores que se repiten. Esto puede ser útil porque tal vez un autor tenga preferencia a escribir noticias de una categoría en particular. Además, un dato no menor es que faltan 24.477 datos de autores. \n",
    "\n",
    "En cuanto a la categoria short_description, podemos observar que en su gran mayoría son distintos, pero hay algunas descripciones que se repiten. Además, también hay un gran número de valores faltantes (13.263).\n",
    "\n",
    "De la columna date podemos decir que hay muchos datos repetidos, ya que solo e 1.7% son valores distintos. Además, no falta ningún valor.\n",
    "\n",
    "Por último, la columna category, es la de la variable a predecir, Hay 41 categorías distintas en todo el set de datos. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llenamos datos faltantes:\n",
    "df_train['headline'].fillna('Unknown headline', inplace=True)\n",
    "df_test['headline'].fillna('Unknown headline', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['short_description'].fillna('Unknown short description', inplace=True)\n",
    "df_test['short_description'].fillna('Unknown short description', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['authors'].fillna('Unknown authors', inplace=True)\n",
    "df_test['authors'].fillna('Unknown authors', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "profile = ProfileReport(df_train)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columna de id\n",
    "# Quitamos la columna id del dataframe porque vimos que era un id autoincremental, el cual era lo mismo que el índice de la fila, por lo tanto, no aportaba información.\n",
    "df_train.drop(columns=['id'], axis=1, inplace=True)\n",
    "df_test.drop(columns=['id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiamos el tipo de los datos a string, ya que antes eran de tipo object, pero en realidad, todos deberían ser strings\n",
    "df_train[['headline', 'authors', 'short_description', 'category', 'date']] = df_train[['headline', 'authors', 'short_description', 'category', 'date']].astype('string')\n",
    "df_test[['headline', 'authors', 'short_description', 'date']] = df_test[['headline', 'authors', 'short_description', 'date']].astype('string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 2309 fechas distintas en el dataset\n",
    "df_train['date'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hay 41 categorias distintas en el dataset\n",
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(80,40))\n",
    "df_train['category'].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Countplot por categoria\n",
    "plt.figure(figsize=(50,5))\n",
    "sns.countplot(data=df_train, x='category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos observar, la mayoría de noticias son de la categoría política"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El histograma nos da una noción global de como es la distribución de \"category\". Los valores más frecuentes son:\n",
    "\n",
    "1 - Politics (21935)\n",
    "\n",
    "2 - Wellness (11944)\n",
    "\n",
    "3 - Entertainment (10759)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VECTORIZACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headlines_corpus = list(df_train['headline'])\n",
    "headlines_corpus_test = list(df_test['headline'])\n",
    "\n",
    "headline_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus)\n",
    "headline_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(headlines_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(headline_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_headlines = headline_vectorizer.transform(headlines_corpus)\n",
    "X_test_headlines = headline_vectorizer_test.transform(headlines_corpus_test)\n",
    "print(X_train_headlines.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de short_description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_description_corpus = list(df_train['short_description'])\n",
    "short_description_corpus_test = list(df_test['short_description'])\n",
    "\n",
    "shortDescription_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus)\n",
    "shortDescription_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True,max_features=1000, max_df=0.80, ).fit(short_description_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shortDescription_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shortDescription_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_short_description = shortDescription_vectorizer.transform(short_description_corpus)\n",
    "X_test_short_description = shortDescription_vectorizer_test.transform(short_description_corpus_test)\n",
    "print(X_train_short_description.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorización de authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_corpus = list(df_train['authors'])\n",
    "authors_corpus_test = list(df_test['authors'])\n",
    "\n",
    "authors_vectorizer = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus)\n",
    "authors_vectorizer_test = CountVectorizer(stop_words='english', dtype=np.int32, lowercase=True, max_features=1000, max_df=0.80).fit(authors_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(authors_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_authors = authors_vectorizer.transform(authors_corpus)\n",
    "X_test_authors = authors_vectorizer_test.transform(authors_corpus_test)\n",
    "print(X_train_authors.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ahora concatenamos los vectores de headlines, short_description y authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "headline_feature_names = ['headline' + elem for elem in headline_vectorizer.get_feature_names()]\n",
    "shortDescription_feature_names = ['shortDescription' + elem for elem in shortDescription_vectorizer.get_feature_names()]\n",
    "authors_feature_names = ['authors' + elem for elem in authors_vectorizer.get_feature_names()]\n",
    "dfHeadline = pd.DataFrame.sparse.from_spmatrix(X_train_headlines, columns=headline_feature_names)\n",
    "dfShortDescription = pd.DataFrame.sparse.from_spmatrix(X_train_short_description, columns=shortDescription_feature_names)\n",
    "dfAuthors = pd.DataFrame.sparse.from_spmatrix(X_train_authors, columns=authors_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "headline_feature_names_test = ['headline' + elem for elem in headline_vectorizer_test.get_feature_names()]\n",
    "shortDescription_feature_names_test = ['shortDescription' + elem for elem in shortDescription_vectorizer_test.get_feature_names()]\n",
    "authors_feature_names_test = ['authors' + elem for elem in authors_vectorizer_test.get_feature_names()]\n",
    "dfHeadline_test = pd.DataFrame.sparse.from_spmatrix(X_test_headlines, columns=headline_feature_names_test)\n",
    "dfShortDescription_test = pd.DataFrame.sparse.from_spmatrix(X_test_short_description, columns=shortDescription_feature_names_test)\n",
    "dfAuthors_test = pd.DataFrame.sparse.from_spmatrix(X_test_authors, columns=authors_feature_names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfHeadline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfShortDescription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfAuthors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.concat([df_train, dfHeadline, dfShortDescription, dfAuthors], axis=1)\n",
    "df_test = pd.concat([df_test, dfHeadline_test, dfShortDescription_test, dfAuthors_test], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos columnas de headline, short_description y authors\n",
    "df_train.drop(columns=['headline', 'short_description', 'authors'], inplace=True)\n",
    "df_test.drop(columns=['headline', 'short_description', 'authors'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label Encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Label encoder para \"date\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateLE = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['date'] = dateLE.fit_transform(df_train['date'])\n",
    "df_test['date'] = dateLE.fit_transform(df_test['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding para \"category\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE = OneHotEncoder(handle_unknown='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = df_train['category'].to_numpy()\n",
    "categories = categories.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = categoryOHE.fit_transform(categories).toarray()\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categoryOHE.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_train.drop(['category'], axis=1)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE ÁRBOL DE DECISIÓN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### De profundidad máxima 3, criterio Gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividimos train en train y dev\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_tree, X_dev_tree, Y_train_tree, Y_devtree = train_test_split(\n",
    "    df_train.drop(['category'], axis=1),\n",
    "    y,\n",
    "    test_size=(0.1), random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arbol.fit(X_train_tree, Y_train_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = arbol.predict(X_dev_tree)\n",
    "Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred, Y_devtree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set to 1 the highest element of an array and the rest to 0\n",
    "def set_to_one(array):\n",
    " newArr = np.zeros(array.shape)\n",
    " newArr[array.argmax()] = 1\n",
    " return newArr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([0.1,0.2,0.3,0.8,0.5])\n",
    "set_to_one(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over the matrix and set to 1 the highest element of each row\n",
    "def set_to_one_row(matrix):\n",
    "\tfor i in range(len(matrix)):\n",
    "\t\trow = matrix[i]\n",
    "\t\tmatrix[i] = set_to_one(row)\n",
    "\treturn matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_then_save(model, X_test, filename):\n",
    "    Y_pred = model.predict(X_test)\n",
    "    print(Y_pred[0])\n",
    "    Y_pred = set_to_one_row(Y_pred)\n",
    "    Y_pred_decoded = categoryOHE.inverse_transform(Y_pred).astype(str).reshape(-1)\n",
    "    with open(filename, 'w') as f:\n",
    "        f.write('id,category\\n')\n",
    "        for i, pred in enumerate(Y_pred_decoded):\n",
    "            f.write(str(i) + ',' + str(pred) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_then_save(arbol, df_test, 'submission_arbol.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARMADO DE RED NEURONAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Dropout, LeakyReLU\n",
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_NN_model(output_shapes, input_shape, use_bias, activations, dropouts, optimizer, loss, metric):\n",
    "    model = Sequential()\n",
    "    layers = len(output_shapes)\n",
    "    for i in range(0, layers):\n",
    "        if i == 0:\n",
    "            model.add(Dense(output_shapes[i], input_shape=(input_shape,), use_bias=use_bias))\n",
    "        else: \n",
    "            model.add(Dense(output_shapes[i], use_bias=use_bias))\n",
    "            model.add(Activation(activations[i]))\n",
    "        if dropouts[i] != None:\n",
    "            model.add(dropouts[i])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=[metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Primera red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = make_NN_model(\n",
    "  output_shapes=[25, 35, Y_train_tree.shape[1]],\n",
    "  input_shape=len(df_train.drop(['category'], axis=1).columns),\n",
    "  use_bias=True,\n",
    "  activations=[LeakyReLU(alpha=0.3), LeakyReLU(alpha=0.3), 'softmax'],\n",
    "  dropouts=[Dropout(0.3), None, None],\n",
    "  optimizer='Adam',\n",
    "  loss='categorical_crossentropy',\n",
    "  metric='accuracy' \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='accuracy', patience=5)\n",
    "model.fit(X_train_tree, Y_train_tree, epochs=2000, validation_split=0.2, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = model.predict(X_dev_tree)\n",
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model = set_to_one_row(Y_pred_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy Arbol:\", accuracy_score(Y_pred_model, Y_devtree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segunda red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "34086a273fcadbdf19105d1b4ac07fbb7b59a732fb2de6e1eab434b7949919a2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('clasemlsi': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
